{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team:\n",
    "\n",
    "- Bertan Karacora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks:\n",
    "\n",
    "- Task 1:\n",
    "    - Implement a **conditional DCGAN** model (https://arxiv.org/abs/1411.1784)\n",
    "    - Train the model for conditional generation on the SVHN dataset\n",
    "    - Requirements:\n",
    "        - Use Tensorboard, WandDB or some other experiment tracker\n",
    "        - Show the capabilities of the model to generate data based on given label\n",
    "     \n",
    "- Task 2:\n",
    "    - Implement a fully convolutional DCGAN-like model (https://arxiv.org/abs/1511.06434)\n",
    "    - Train the model on the CelebA dataset to generate new faces\n",
    "    - Requirements:\n",
    "        - Use Tensorboard, WandDB or some other experiment tracker\n",
    "        - Show the capabilities of your model to generate images\n",
    "        - Evaluate and track during training using one quantitative metric (e.g. FID)\n",
    " \n",
    "- Extra point:\n",
    "    - Train a SAGAN (self-attention GAN, (https://arxiv.org/abs/1805.08318) or BigGAN (https://arxiv.org/abs/1809.11096)) model on the CelebA dataset\n",
    "    - You are allowed to use open-source implementations\n",
    "    - Compare, both qualitatively and quantitatively, SAGAN/BigGAN with the DCGAN from task 2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] [Setup](#setup)\n",
    "    - [x] [Config](#setup_config)\n",
    "    - [x] [Modules](#setup_modules)\n",
    "    - [x] [Paths and names](#setup_paths_and_names)\n",
    "- [x] [Data](#data)\n",
    "    - [x] [Visualization](#data_visualization)\n",
    "        - [x] [SVHN](#data_visualization_svhn)\n",
    "        - [x] [CelebA](#data_visualization_celeba)\n",
    "    - [x] [Remarks](#data_remarks)\n",
    "- [x] [Models](#models)\n",
    "    - [x] [DCGAN](#models_dcgan)\n",
    "    - [x] [CDCGAN](#models_cdcgan)\n",
    "- [x] [Experiments](#experiments)\n",
    "    - [x] [CDCGAN on SVHN](#experiments_svhn_cdcgan)\n",
    "    - [x] [DCGAN on CelebA](#experiments_celeba_dcgan)\n",
    "    - [x] [Discussion](#experiments_discussion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "<a id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config\n",
    "<a id=\"setup_config\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded from /home/user/karacora/lab-vision-systems-assignments/assignment_6/assignment/config.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['celeba_dcgan', 'celeba_unnormalized', 'svhn_cdcgan', 'svhn_unnormalized']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import assignment.config as config\n",
    "\n",
    "config.list_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules\n",
    "<a id=\"setup_modules\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torchsummary\n",
    "\n",
    "from assignment.evaluation.evaluator import Evaluator\n",
    "import assignment.libs.utils_checkpoints as utils_checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths and names\n",
    "<a id=\"setup_paths_and_names\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_exp_svhn_unnormalized = \"svhn_unnormalized\"\n",
    "name_exp_celeba_unnormalized = \"celeba_unnormalized\"\n",
    "name_exp_svhn_cdcgan = \"svhn_cdcgan\"\n",
    "name_exp_celeba_dcgan = \"celeba_dcgan\"\n",
    "\n",
    "path_dir_exp_svhn_cdcgan = Path(config._PATH_DIR_EXPS) / name_exp_svhn_cdcgan\n",
    "path_dir_exp_celeba_dcgan = Path(config._PATH_DIR_EXPS) / name_exp_celeba_dcgan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "<a id=\"data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "<a id=\"data_visualization\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVHN\n",
    "<a id=\"data_visualization_svhn\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Test dataset](experiments/svhn_cdcgan/visualizations/Sample_test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Validate dataset](experiments/svhn_cdcgan/visualizations/Sample_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Training dataset](experiments/svhn_cdcgan/visualizations/Sample_training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training dataset (normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Training dataset (normalized)](experiments/svhn_cdcgan/visualizations/Sample_training_normalized.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CelebA\n",
    "<a id=\"data_visualization_celeba\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Test dataset](experiments/celeba_dcgan/visualizations/Sample_test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Validate dataset](experiments/celeba_dcgan/visualizations/Sample_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Training dataset](experiments/celeba_dcgan/visualizations/Sample_training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training dataset (normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Training dataset (normalized)](experiments/celeba_dcgan/visualizations/Sample_training_normalized.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "<a id=\"data_remarks\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Implementation:\n",
    ">\n",
    "> - [SCHN dataset class](assignment/datasets/schn.py)\n",
    "> - [CelebA dataset class](assignment/datasets/celeba.py)\n",
    "> - [Script for computing mean and standard deviation of training dataset](assignment/scripts/compute_mean_and_std.py)\n",
    "\n",
    "> Remarks:\n",
    "> \n",
    "> - No data augmentation is used. As in the previous assignment, most augmentations would be unreasonable when training generative momdels. At most, some slight spatial augmentations might be useful to create additional data samples, but the datasets are large enough.\n",
    "> - CelebA: Original images of shape $178 \\times 218$ are cropped $178 \\times 178$. Subsequently, they are rezised to $128 \\times 128$.\n",
    "> - The images are normalized using the mean and standard deviation of the training dataset. The normalization is applied also during validation and inference.\n",
    "> - Since we normalize the images, we need to pay attention to do the same for any generated images (first mapping to [0, 1], then normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "<a id=\"models\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCGAN\n",
    "<a id=\"models_dcgan\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded from /home/user/karacora/lab-vision-systems-assignments/assignment_6/experiments/celeba_dcgan/config.yaml\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 3, 128, 128]         --\n",
      "|    └─BlockConvTranspose2d: 2-1         [-1, 1024, 2, 2]          --\n",
      "|    |    └─ConvTranspose2d: 3-1         [-1, 1024, 2, 2]          2,098,176\n",
      "|    |    └─BatchNorm2d: 3-2             [-1, 1024, 2, 2]          2,048\n",
      "|    |    └─ReLU: 3-3                    [-1, 1024, 2, 2]          --\n",
      "|    └─BlockConvTranspose2d: 2-2         [-1, 512, 4, 4]           --\n",
      "|    |    └─ConvTranspose2d: 3-4         [-1, 512, 4, 4]           8,389,120\n",
      "|    |    └─BatchNorm2d: 3-5             [-1, 512, 4, 4]           1,024\n",
      "|    |    └─ReLU: 3-6                    [-1, 512, 4, 4]           --\n",
      "|    └─BlockConvTranspose2d: 2-3         [-1, 256, 8, 8]           --\n",
      "|    |    └─ConvTranspose2d: 3-7         [-1, 256, 8, 8]           2,097,408\n",
      "|    |    └─BatchNorm2d: 3-8             [-1, 256, 8, 8]           512\n",
      "|    |    └─ReLU: 3-9                    [-1, 256, 8, 8]           --\n",
      "|    └─BlockConvTranspose2d: 2-4         [-1, 128, 16, 16]         --\n",
      "|    |    └─ConvTranspose2d: 3-10        [-1, 128, 16, 16]         524,416\n",
      "|    |    └─BatchNorm2d: 3-11            [-1, 128, 16, 16]         256\n",
      "|    |    └─ReLU: 3-12                   [-1, 128, 16, 16]         --\n",
      "|    └─BlockConvTranspose2d: 2-5         [-1, 64, 32, 32]          --\n",
      "|    |    └─ConvTranspose2d: 3-13        [-1, 64, 32, 32]          131,136\n",
      "|    |    └─BatchNorm2d: 3-14            [-1, 64, 32, 32]          128\n",
      "|    |    └─ReLU: 3-15                   [-1, 64, 32, 32]          --\n",
      "|    └─BlockConvTranspose2d: 2-6         [-1, 32, 64, 64]          --\n",
      "|    |    └─ConvTranspose2d: 3-16        [-1, 32, 64, 64]          32,800\n",
      "|    |    └─BatchNorm2d: 3-17            [-1, 32, 64, 64]          64\n",
      "|    |    └─ReLU: 3-18                   [-1, 32, 64, 64]          --\n",
      "|    └─BlockConvTranspose2d: 2-7         [-1, 3, 128, 128]         --\n",
      "|    |    └─ConvTranspose2d: 3-19        [-1, 3, 128, 128]         1,539\n",
      "|    |    └─Tanh: 3-20                   [-1, 3, 128, 128]         --\n",
      "==========================================================================================\n",
      "Total params: 13,278,627\n",
      "Trainable params: 13,278,627\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 731.19\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 4.31\n",
      "Params size (MB): 50.65\n",
      "Estimated Total Size (MB): 54.97\n",
      "==========================================================================================\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 1, 1, 1]             --\n",
      "|    └─BlockConv2d: 2-1                  [-1, 32, 64, 64]          --\n",
      "|    |    └─Conv2d: 3-1                  [-1, 32, 64, 64]          1,568\n",
      "|    |    └─BatchNorm2d: 3-2             [-1, 32, 64, 64]          64\n",
      "|    |    └─LeakyReLU: 3-3               [-1, 32, 64, 64]          --\n",
      "|    |    └─Dropout: 3-4                 [-1, 32, 64, 64]          --\n",
      "|    └─BlockConv2d: 2-2                  [-1, 64, 32, 32]          --\n",
      "|    |    └─Conv2d: 3-5                  [-1, 64, 32, 32]          32,832\n",
      "|    |    └─BatchNorm2d: 3-6             [-1, 64, 32, 32]          128\n",
      "|    |    └─LeakyReLU: 3-7               [-1, 64, 32, 32]          --\n",
      "|    |    └─Dropout: 3-8                 [-1, 64, 32, 32]          --\n",
      "|    └─BlockConv2d: 2-3                  [-1, 128, 16, 16]         --\n",
      "|    |    └─Conv2d: 3-9                  [-1, 128, 16, 16]         131,200\n",
      "|    |    └─BatchNorm2d: 3-10            [-1, 128, 16, 16]         256\n",
      "|    |    └─LeakyReLU: 3-11              [-1, 128, 16, 16]         --\n",
      "|    |    └─Dropout: 3-12                [-1, 128, 16, 16]         --\n",
      "|    └─BlockConv2d: 2-4                  [-1, 256, 8, 8]           --\n",
      "|    |    └─Conv2d: 3-13                 [-1, 256, 8, 8]           524,544\n",
      "|    |    └─BatchNorm2d: 3-14            [-1, 256, 8, 8]           512\n",
      "|    |    └─LeakyReLU: 3-15              [-1, 256, 8, 8]           --\n",
      "|    |    └─Dropout: 3-16                [-1, 256, 8, 8]           --\n",
      "|    └─BlockConv2d: 2-5                  [-1, 512, 4, 4]           --\n",
      "|    |    └─Conv2d: 3-17                 [-1, 512, 4, 4]           2,097,664\n",
      "|    |    └─BatchNorm2d: 3-18            [-1, 512, 4, 4]           1,024\n",
      "|    |    └─LeakyReLU: 3-19              [-1, 512, 4, 4]           --\n",
      "|    |    └─Dropout: 3-20                [-1, 512, 4, 4]           --\n",
      "|    └─BlockConv2d: 2-6                  [-1, 1024, 2, 2]          --\n",
      "|    |    └─Conv2d: 3-21                 [-1, 1024, 2, 2]          8,389,632\n",
      "|    |    └─BatchNorm2d: 3-22            [-1, 1024, 2, 2]          2,048\n",
      "|    |    └─LeakyReLU: 3-23              [-1, 1024, 2, 2]          --\n",
      "|    |    └─Dropout: 3-24                [-1, 1024, 2, 2]          --\n",
      "|    └─BlockConv2d: 2-7                  [-1, 1, 1, 1]             --\n",
      "|    |    └─Conv2d: 3-25                 [-1, 1, 1, 1]             16,385\n",
      "|    |    └─Sigmoid: 3-26                [-1, 1, 1, 1]             --\n",
      "==========================================================================================\n",
      "Total params: 11,197,857\n",
      "Trainable params: 11,197,857\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 196.47\n",
      "==========================================================================================\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 3.94\n",
      "Params size (MB): 42.72\n",
      "Estimated Total Size (MB): 46.84\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "config.set_config_exp(path_dir_exp_celeba_dcgan)\n",
    "\n",
    "model_generator = utils_checkpoints.load_model_generator(path_dir_exp_celeba_dcgan / \"checkpoints\" / \"latest.pth\")\n",
    "model_discriminator = utils_checkpoints.load_model_discriminator(path_dir_exp_celeba_dcgan / \"checkpoints\" / \"latest.pth\")\n",
    "\n",
    "print(torchsummary.summary(model_generator, [config.MODEL_GENERATOR[\"shape_input\"]], verbose=0))\n",
    "print(torchsummary.summary(model_discriminator, [config.MODEL_DISCRIMINATOR[\"shape_input\"]], verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDCGAN\n",
    "<a id=\"models_cdcgan\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded from /home/user/karacora/lab-vision-systems-assignments/assignment_6/experiments/svhn_cdcgan/config.yaml\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─BlockConvTranspose2d: 1-1              [-1, 512, 2, 2]           --\n",
      "|    └─ConvTranspose2d: 2-1              [-1, 512, 2, 2]           262,656\n",
      "|    └─BatchNorm2d: 2-2                  [-1, 512, 2, 2]           1,024\n",
      "|    └─ReLU: 2-3                         [-1, 512, 2, 2]           --\n",
      "├─BlockConvTranspose2d: 1-2              [-1, 512, 2, 2]           --\n",
      "|    └─ConvTranspose2d: 2-4              [-1, 512, 2, 2]           8,704\n",
      "|    └─BatchNorm2d: 2-5                  [-1, 512, 2, 2]           1,024\n",
      "|    └─ReLU: 2-6                         [-1, 512, 2, 2]           --\n",
      "├─Sequential: 1-3                        [-1, 3, 32, 32]           --\n",
      "|    └─BlockConvTranspose2d: 2-7         [-1, 512, 4, 4]           --\n",
      "|    |    └─ConvTranspose2d: 3-1         [-1, 512, 4, 4]           8,389,120\n",
      "|    |    └─BatchNorm2d: 3-2             [-1, 512, 4, 4]           1,024\n",
      "|    |    └─ReLU: 3-3                    [-1, 512, 4, 4]           --\n",
      "|    └─BlockConvTranspose2d: 2-8         [-1, 256, 8, 8]           --\n",
      "|    |    └─ConvTranspose2d: 3-4         [-1, 256, 8, 8]           2,097,408\n",
      "|    |    └─BatchNorm2d: 3-5             [-1, 256, 8, 8]           512\n",
      "|    |    └─ReLU: 3-6                    [-1, 256, 8, 8]           --\n",
      "|    └─BlockConvTranspose2d: 2-9         [-1, 128, 16, 16]         --\n",
      "|    |    └─ConvTranspose2d: 3-7         [-1, 128, 16, 16]         524,416\n",
      "|    |    └─BatchNorm2d: 3-8             [-1, 128, 16, 16]         256\n",
      "|    |    └─ReLU: 3-9                    [-1, 128, 16, 16]         --\n",
      "|    └─BlockConvTranspose2d: 2-10        [-1, 3, 32, 32]           --\n",
      "|    |    └─ConvTranspose2d: 3-10        [-1, 3, 32, 32]           6,147\n",
      "|    |    └─Tanh: 3-11                   [-1, 3, 32, 32]           --\n",
      "==========================================================================================\n",
      "Total params: 11,292,291\n",
      "Trainable params: 11,292,291\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 432.33\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.96\n",
      "Params size (MB): 43.08\n",
      "Estimated Total Size (MB): 44.04\n",
      "==========================================================================================\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─BlockConvTranspose2d: 1-1              [-1, 512, 2, 2]           --\n",
      "|    └─ConvTranspose2d: 2-1              [-1, 512, 2, 2]           262,656\n",
      "|    └─BatchNorm2d: 2-2                  [-1, 512, 2, 2]           1,024\n",
      "|    └─ReLU: 2-3                         [-1, 512, 2, 2]           --\n",
      "├─BlockConvTranspose2d: 1-2              [-1, 512, 2, 2]           --\n",
      "|    └─ConvTranspose2d: 2-4              [-1, 512, 2, 2]           8,704\n",
      "|    └─BatchNorm2d: 2-5                  [-1, 512, 2, 2]           1,024\n",
      "|    └─ReLU: 2-6                         [-1, 512, 2, 2]           --\n",
      "├─Sequential: 1-3                        [-1, 3, 32, 32]           --\n",
      "|    └─BlockConvTranspose2d: 2-7         [-1, 512, 4, 4]           --\n",
      "|    |    └─ConvTranspose2d: 3-1         [-1, 512, 4, 4]           8,389,120\n",
      "|    |    └─BatchNorm2d: 3-2             [-1, 512, 4, 4]           1,024\n",
      "|    |    └─ReLU: 3-3                    [-1, 512, 4, 4]           --\n",
      "|    └─BlockConvTranspose2d: 2-8         [-1, 256, 8, 8]           --\n",
      "|    |    └─ConvTranspose2d: 3-4         [-1, 256, 8, 8]           2,097,408\n",
      "|    |    └─BatchNorm2d: 3-5             [-1, 256, 8, 8]           512\n",
      "|    |    └─ReLU: 3-6                    [-1, 256, 8, 8]           --\n",
      "|    └─BlockConvTranspose2d: 2-9         [-1, 128, 16, 16]         --\n",
      "|    |    └─ConvTranspose2d: 3-7         [-1, 128, 16, 16]         524,416\n",
      "|    |    └─BatchNorm2d: 3-8             [-1, 128, 16, 16]         256\n",
      "|    |    └─ReLU: 3-9                    [-1, 128, 16, 16]         --\n",
      "|    └─BlockConvTranspose2d: 2-10        [-1, 3, 32, 32]           --\n",
      "|    |    └─ConvTranspose2d: 3-10        [-1, 3, 32, 32]           6,147\n",
      "|    |    └─Tanh: 3-11                   [-1, 3, 32, 32]           --\n",
      "==========================================================================================\n",
      "Total params: 11,292,291\n",
      "Trainable params: 11,292,291\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 432.33\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.96\n",
      "Params size (MB): 43.08\n",
      "Estimated Total Size (MB): 44.04\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "config.set_config_exp(path_dir_exp_svhn_cdcgan)\n",
    "\n",
    "model_generator = utils_checkpoints.load_model_generator(path_dir_exp_svhn_cdcgan / \"checkpoints\" / \"latest.pth\")\n",
    "model_discriminator = utils_checkpoints.load_model_discriminator(path_dir_exp_svhn_cdcgan / \"checkpoints\" / \"latest.pth\")\n",
    "\n",
    "print(torchsummary.summary(model_generator, [config.MODEL_GENERATOR[\"shape_input\"], [1]], verbose=0))\n",
    "print(torchsummary.summary(model_generator, [config.MODEL_GENERATOR[\"shape_input\"], [1]], verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "<a id=\"models_remarks\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Implementation:\n",
    ">\n",
    "> - [Training configs](assignment/configs)\n",
    "> - [DCGAN model](assignment/models/gan.py)\n",
    "> - [CDCGAN model class](assignment/models/gan.py)\n",
    "> - [Convolutional blocks](assignment/models/cnn.py)\n",
    "\n",
    "> Remarks:\n",
    "> \n",
    "> - The discriminator and generator are almost symmetric, most importnatly, they share the same dimensions (in opoosite order) for the hidden layers.\n",
    "> - Each block double/halves the image shape. As CelebA consists of $3 \\times 128 \\times 128$ images and SVHN of $3 \\times 32 \\times 32$ images, the configured DCGAN uses 6 Blocks both for the discriminator and generator. The CDCGAN for SVHN uses 4 Blocks, just like in the original publication.\n",
    "> - The models are fully convolutional, no linear layers are used.\n",
    "> - For the bottleneck, CelebA latent codes contain $128$ scalars, representing images of original input size $3 \\cdot 128 \\cdot 128 = 49152$. SVHN latent codes contain $32$ scalars, representing images of original input size $3 \\cdot 32 \\cdot 32 = 3072$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "<a id=\"experiments\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDCGAN on SVHN\n",
    "<a id=\"experiments_svhn_cdcgan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Loss](experiments/svhn_cdcgan/plots/Loss_discriminator.png)\n",
    "![Learning rate](experiments/svhn_cdcgan/plots/Learning_rate_discriminator.png)\n",
    "![Metrics_BinaryAccuracy_discriminator](experiments/svhn_cdcgan/plots/Metrics_BinaryAccuracy_discriminator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Loss](experiments/svhn_cdcgan/plots/Loss_generator.png)\n",
    "![Learning rate](experiments/svhn_cdcgan/plots/Learning_rate_generator.png)\n",
    "![FrechetInceptionDistance](experiments/svhn_cdcgan/plots/Metrics_FrechetInceptionDistance_generator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Samples generated unnormalized](experiments/svhn_cdcgan/visualizations/Samples_generated_unnormalized.png)\n",
    "![Interpolation grid](experiments/svhn_cdcgan/visualizations/Interpolation_grid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCGAN on CelebA\n",
    "<a id=\"experiments_celeba_dcgan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Loss](experiments/celeba_dcgan/plots/Loss_discriminator.png)\n",
    "![Learning rate](experiments/celeba_dcgan/plots/Learning_rate_discriminator.png)\n",
    "![Metrics_BinaryAccuracy_discriminator](experiments/celeba_dcgan/plots/Metrics_BinaryAccuracy_discriminator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Loss](experiments/celeba_dcgan/plots/Loss_generator.png)\n",
    "![Learning rate](experiments/celeba_dcgan/plots/Learning_rate_generator.png)\n",
    "![FrechetInceptionDistance](experiments/celeba_dcgan/plots/Metrics_FrechetInceptionDistance_generator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Samples generated unnormalized](experiments/celeba_dcgan/visualizations/Samples_generated_unnormalized.png)\n",
    "![Interpolation grid](experiments/celeba_dcgan/visualizations/Interpolation_grid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "<a id=\"comparison_of_recurrent_models_discussion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Implementation:\n",
    ">\n",
    "> - [Configs](assignment/configs)\n",
    "> - [Trainer](assignment/training/trainer_gan.py)\n",
    "> - [Jupyter notebook for CelebA experiment](assignment_6_run_celeba.ipynb)\n",
    "> - [Jupyter notebook for SVHN experiment](assignment_6_run_svhn.ipynb)\n",
    ">\n",
    "\n",
    "> Some remarks:\n",
    ">\n",
    "> - All models have been trained for 20 epochs. Training did take a lot of time as the dataset is very large, the models are medium sized, and some operations like visualizing generated images in tensorboard or evaluating the FID score are somewhat costly.\n",
    "> - A warmup + exponential decay scheduler has been used. Since I intended to run the training for 20 epochs, the decay is rather strong.\n",
    "> - The Frechet Inception Distance (FID) is rather costly to evaluate but it shows the training progress nicely.\n",
    ">\n",
    "\n",
    "> Results:\n",
    ">\n",
    "> Learnable params refers to the generator model. FID on the training dataset:\n",
    "\n",
    "    | Model             |   FID    | learnable params | Total mult-adds (M): |\n",
    "    | :---------------- | :------: | :--------------: | ------------------:  |\n",
    "    | DCGAN on CelebA   |  3.823   |    13,278,627    |    731.19            |\n",
    "    | CDCGAN on SVHN    |  0.493   |    11,292,291    |    432.33            |\n",
    "\n",
    "\n",
    "> Observations/Conclusion:\n",
    ">\n",
    "> - In both experiments, I would say that reasonable results have been obtained.\n",
    "> - Generally, the loss curves do not tell us too much. If one model performs better the loss increases for the other. Therefore, we might see wild changes and stagnation instead of a smooth progression. Also, it is probably safe to say that the discriminator has the easier task, which is consistent with the high binary classification Accuracy of $0.8$ most of the time.\n",
    "> - The FID score for the CDCGAN on the SVHN dataset shows a nice and smooth progression over the 20 training epochs. The generated samples for the number 0 are not clearly identifyable as this number, however, arguably, the images resemble the digit 0 most among all digits.\n",
    "> - The CelebA images are larger and much more complex (face vs digit). Therefore, a worse generation performance is to be expected. The results are consistent with this assumption.\n",
    "> - The generated CelebA face images clearly show different human faces. However, the quality not as good that fake and real images would be indistinguishable for a human (which I also would not expect here). One can say, however, that the images do show some complex facial traits. One can also see that some depicted persons are smiling, looking in different directions, having different backgrounds, hairstyles, etc. Some additional regularization and hyperparameter tuning might already be sufficient to improve the performance further.\n",
    "> - Possibly, one can see some rectagular artifacts in the generated images. These might be the consequence of the strided transposed convolutions that have been used to generate them. A deeper network with less stride distances and/or pooling and linear layers might be able to mitigate this effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
