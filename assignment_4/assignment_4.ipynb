{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team:\n",
    "\n",
    "- Bertan Karacora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks:\n",
    "\n",
    "- For your experiments, use at least one augmentation from each of the following types:\n",
    "    - Spatial Augmentations (rotation, mirroring, croppoing, ...)\n",
    "    - Use some other augmentations (color jitter, gaussian noise, ...).\n",
    "    - Use one (or more) of the following advanced augmentations:\n",
    "    - **CutMix**: https://arxiv.org/pdf/1905.04899.pdf\n",
    "    - **Mixup**: https://arxiv.org/pdf/1710.09412.pdf\n",
    "\n",
    "- **Experiments 1:** Using your aforementioned augmentions:\n",
    "    - Fine-tune ResNet, MobileNet, and ConvNext for your augmented dataset for car type classification and compare them.\n",
    "    - Compare the following on a model of your choice: Fine-Tuned model, model as fixed feature extractor, and model with a Combined Approach\n",
    "    - Log your losses and accuracies into Tensorboard (or some other logging tool)\n",
    "    - **Extra Point**:\n",
    "        - Fine-tune a Transformer-based model (e.g. SwinTransformer). Compare the performance (accuracy, confusion matrix, training time, loss landscape, ...) with the one from the convolutional models.\n",
    "   \n",
    "- **Experiment 2:** Try to get the best performance possible on this dataset\n",
    "    - Fine-tune a pretrained neural network of your choice for classification.\n",
    "    - Select a good training recipe: augmentations, optimizer, learning rate scheduling, classifier, loss function, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] [Setup](#setup)\n",
    "    - [x] [Config](#setup_config)\n",
    "    - [x] [Modules](#setup_modules)\n",
    "    - [x] [Paths and names](#setup_paths_and_names)\n",
    "- [x] [Data augmentation](#data_augmentation)\n",
    "    - [x] [Visualization](#data_augmentation_visualization)\n",
    "    - [x] [Discussion](#data_augmentation_discussion)\n",
    "- [x] [Comparison of fine-tuned models](#comparison_of_fine_tuned_models)\n",
    "    - [x] [ResNet](#comparison_of_fine_tuned_models_resnet)\n",
    "    - [x] [MobileNet](#comparison_of_fine_tuned_models_mobilenet)\n",
    "    - [x] [ConvNext](#comparison_of_fine_tuned_models_convnext)\n",
    "    - [x] [Discussion](#comparison_of_fine_tuned_models_discussion)\n",
    "- [x] [Comparison of transfer learning approaches](#comparison_of_transfer_learning_approaches)\n",
    "    - [x] [Fixed feature extraction](#comparison_of_transfer_learning_approaches_fixed_feature_extraction)\n",
    "    - [x] [Fine-tuning](#comparison_of_transfer_learning_approaches_fine_tuning)\n",
    "    - [x] [Combined approach](#comparison_of_transfer_learning_approaches_combined_approach)\n",
    "    - [x] [Discussion](#comparison_of_transfer_learning_approaches_discussion)\n",
    "- [x] [Tensorboard](#tensorboard)\n",
    "    - [x] [Visualization](#tensorboard_visualization)\n",
    "    - [x] [Discussion](#tensorboard_discussion)\n",
    "- [ ] [Fine-tuning a transformer-based model](#fine_tuning_a_transformer_based_model)\n",
    "    - [ ] [Training and evaluation](#fine_tuning_a_transformer_based_model_training_and_evaluation)\n",
    "    - [ ] [Discussion](#fine_tuning_a_transformer_based_model_discussion)\n",
    "- [ ] [Car type classification](#car_type_classification)\n",
    "    - [ ] [Training and evaluation](#car_type_classification_training_and_evaluation)\n",
    "    - [ ] [Discussion](#car_type_classification_discussion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LSTM implementation](assignment/models/lstm.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Test dataset](experiments/kth_lstm/visualizations/Images_test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Validate dataset](experiments/kth_lstm/visualizations/Images_validate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Train dataset](experiments/kth_lstm/visualizations/Images_train.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "<a id=\"data_augmentation_discussion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Besides data type conversion and normalization, the following augmentations are applied in all experiments:\n",
    ">\n",
    "> - Random cropping (and then resizing the resulting patch).\n",
    "> - Random horizontal flip with probability $p=0.5$. Vertical flip would not be sensible.\n",
    "> - Random rotation by a degree $d \\sim [-20, 20]$. This seems like a realistic and efficient way to augment images.\n",
    "> - Brightness jitter by a factor $f \\sim [0.6, 1.4]$. Brightness would be a factor with high variability in real world photographs (e.g., depending on daytime).\n",
    "> - Contrast jitter by a factor $f \\sim [0.8, 1.2]$.\n",
    "> - Saturation jitter by a factor $f \\sim [0.9, 1.1]$.\n",
    "> - Hue jitter by a factor $f \\sim [-0.2, 0.2]$. Colors of cars are pretty much arbitrary so it makes sense to use this augmentation.\n",
    "> - Gaussian noise with mean $0.0$ and standard deviation $0.05$. Since this actually affects the intensity ranges, they are clipped back to the interval $[0.0, 1.0]$ afterards. This is done in another transform.\n",
    "> - MixUp with interpolating value according to beta distribution where $\\alpha=\\beta=1.0$.\n",
    ">\n",
    "> Some transforms that are not available in Torchvision have been implemented in `assignment/transforms/`.\n",
    "> The config file of each experiment is used to define these parameters.\n",
    "> CutMix has been tested, but MixUp seems to fulfill the same purpose in a smoother way by interpolating instead of doing unrealistic cuts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_, model, _, _ = utils_checkpoints.load(path_dir_exp_lstm / \"checkpoints\" / \"final.pth\")\n",
    "\n",
    "evaluator = Evaluator(name_exp_lstm, model)\n",
    "evaluator.evaluate()\n",
    "\n",
    "print(f\"Loss on test data: {evaluator.log[\"total\"][\"loss\"]}\")\n",
    "print(f\"Metrics on test data\")\n",
    "for name, metrics in evaluator.log[\"total\"][\"metrics\"].items():\n",
    "    print(f\"    {name:<10}: {metrics}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
